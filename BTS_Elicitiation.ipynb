{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boom90lb/BTSElicitation/blob/main/BTS_Elicitiation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TaJcGngC6c8i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLmu6-J_qJwL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f11ee6d4-4baa-40d2-9e5f-248f7de7bb60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\n",
            "Requirement already satisfied: auto-gptq in /usr/local/lib/python3.10/dist-packages (0.5.1+cu118)\n",
            "Requirement already satisfied: accelerate>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.25.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.15.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.1.99)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.23.5)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.0.1)\n",
            "Requirement already satisfied: gekko in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.0.6)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.1.0+cu118)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.4.1)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.36.0.dev0)\n",
            "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->auto-gptq) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->auto-gptq) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->auto-gptq) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->auto-gptq) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (0.15.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.9.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.7)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: icecream in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: optimum in /usr/local/lib/python3.10/dist-packages (1.14.1)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: colorama>=0.3.9 in /usr/local/lib/python3.10/dist-packages (from icecream) (0.4.6)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from icecream) (2.16.1)\n",
            "Requirement already satisfied: executing>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from icecream) (2.0.1)\n",
            "Requirement already satisfied: asttokens>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from icecream) (2.4.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from optimum) (15.0.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum) (1.12)\n",
            "Requirement already satisfied: transformers[sentencepiece]>=4.26.0 in /usr/local/lib/python3.10/dist-packages (from optimum) (4.36.0.dev0)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from optimum) (2.1.0+cu118)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.0.1->icecream) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum) (0.4.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum) (3.20.3)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->optimum) (10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->optimum) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "if 1 == 1:\n",
        "  !pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "  !pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n",
        "  !pip install openai datasets icecream optimum"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{\n",
        "  \"question\": \"What is the embryological origin of the hyoid bone?\",\n",
        "  \"choices\": [\"The first pharyngeal arch\", \"The first and second pharyngeal arches\", \"The second pharyngeal arch\", \"The second and third pharyngeal arches\"],\n",
        "  \"answer\": \"D\"\n",
        "}\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "P4OtXjrVoA0Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djR1bvi1q-iM"
      },
      "outputs": [],
      "source": [
        "import abc\n",
        "from icecream import ic\n",
        "from typing import List, Dict\n",
        "\n",
        "import torch\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import re\n",
        "import datasets\n",
        "import huggingface_hub\n",
        "from huggingface_hub import InferenceApi  # Ensure this import is present\n",
        "import transformers\n",
        "\n",
        "\n",
        "dataset = datasets.load_dataset(\"truthful_qa\", \"multiple_choice\")[\"validation\"]\n",
        "\n",
        "import openai\n",
        "model = \"openai\"\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "\n",
        "import requests\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcwLleCrqHpR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7682f74-4358-470d-fb92-865d2b2c942b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['question', 'subject', 'choices', 'answer'],\n",
            "    num_rows: 390\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "dataset_MMLU = datasets.load_dataset(\"cais/mmlu\", \"high_school_macroeconomics\")[\"test\"]\n",
        "\n",
        "question_idx = 1\n",
        "print(dataset_MMLU)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = load_dataset(\"cais/mmlu\")\n",
        "# dataset_MMLU1 = datasets.load_dataset(\"lukaemon/mmlu\")[\"validation\"]\n",
        "\n",
        "# Assuming dataset_MMLU is a Dataset object from the 'datasets' library\n",
        "# question_idx = 2\n",
        "# question = dataset_MMLU[question_idx]['question']\n",
        "# answer_choices = dataset_MMLU[question_idx]['choices']\n",
        "# target = dataset_MMLU[question_idx]['answer']\n",
        "\n",
        "# print(target)"
      ],
      "metadata": {
        "id": "OTnw-DO3p1Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VchTHdK5sVa-"
      },
      "outputs": [],
      "source": [
        "def generate_prompt_answer(question, answer_choices, COT):\n",
        "    # Joining the answer choices into a formatted string\n",
        "    answer_choice_text = \"\\n\".join(\n",
        "        f\"({chr(97 + i)}) {answer}\" for i, answer in enumerate(answer_choices)\n",
        "    )\n",
        "\n",
        "    # Constructing the prompt\n",
        "    prompt = (\n",
        "        f\"\"\"We are asking you questions. You try to respond truthfully. We give you a question and some answer options.\\n\\n\"\n",
        "        You provide us with an answer in the form of probabilities.\\n\n",
        "        Example:\\n\n",
        "        (a) p%\\n\n",
        "        (b) q%\\n\n",
        "        (c) r%\\n\n",
        "        ...\n",
        "        Now you will be provided the question and answer choices.\\n\n",
        "        Question: {question}\\n\n",
        "        {answer_choice_text}\\n\n",
        "        Please respond with your probability distribution over answers.\n",
        "        Please first reason out for {COT} sentences.\"\"\"\n",
        "    )\n",
        "\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYh4RWeyrBNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7bc10e1-75f6-470b-80c5-53f606768c50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The list is valid.\n"
          ]
        }
      ],
      "source": [
        "def generate_prompts_pred(question, answer_choices, COT = 1):\n",
        "    prompts_predictor = []\n",
        "    number_of_choices = len(answer_choices)\n",
        "\n",
        "    for i in range(number_of_choices):\n",
        "        prompt_pred = f\"\"\"Interviewer: We are asking you questions. You try to respond truthfully. We give you a question and some answer options.\n",
        "\n",
        "You provide us with your best answer. Now, you will be provided the question and some answer choices.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "{', '.join([f'({chr(97 + j)}) {choice}' for j, choice in enumerate(answer_choices)])}\n",
        "\n",
        "Please respond with your answer.\n",
        "\n",
        "AI Agent: Sure, I can respond truthfully and with the best of my knowledge. I think the answer is {chr(97 + i)}, that is: {answer_choices[i]}.\n",
        "\n",
        "Interviewer: Thank you so much. Next, we will want to know your predicted probability distribution over answers that others would give. You will be rewarded based on a strictly proper scoring rule.\n",
        "\n",
        "The format of your answer would be like that:\n",
        "\n",
        "\n",
        "Please provide your predictions in this format.\n",
        "(a) p%\n",
        "(b) q%\n",
        "...\n",
        "\n",
        "Please first reason step by step in {COT} sentences.\n",
        "\n",
        "AI Agent: \"\"\"\n",
        "        prompts_predictor.append(prompt_pred)\n",
        "\n",
        "    return prompts_predictor\n",
        "\n",
        "tokenizer, model = None, None\n",
        "\n",
        "\n",
        "def assert_list_of_strings(input_list):\n",
        "    # Check if the input is a list\n",
        "    assert isinstance(input_list, list), \"Input is not a list\"\n",
        "\n",
        "    # Check each element in the list to ensure it's a string\n",
        "    for element in input_list:\n",
        "        assert isinstance(element, str), \"Element is not a string\"\n",
        "\n",
        "# Example usage\n",
        "try:\n",
        "    test_list = [\"string1\", \"string2\", \"string3\"]\n",
        "    assert_list_of_strings(test_list)\n",
        "    print(\"The list is valid.\")\n",
        "except AssertionError as e:\n",
        "    print(f\"AssertionError: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "def call_llm(prompts, model):\n",
        "        if model == \"manual\":\n",
        "            return prompts\n",
        "        if model == \"openai\":\n",
        "            response = openai.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo-1106\",  # or another model of your choice\n",
        "                # model=\"gpt-4\",  # or another model of your choice\n",
        "\n",
        "            messages = {\"role\": \"system\", \"content\": prompts}\n",
        "            )\n",
        "\n",
        "            gen_texts = response\n",
        "            return gen_texts\n",
        "\n",
        "        # if model == \"llama\":\n",
        "        if model == \"huggingface\":\n",
        "          if tokenizer is None:\n",
        "            tokenizer = transformers.AutoTokenizer.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\", use_fast=True)\n",
        "            model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "              \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\",\n",
        "              device_map=\"cuda\",\n",
        "              revision=\"gptq-4bit-32g-actorder_True\"\n",
        "            )\n",
        "\n",
        "          # Set up the Hugging Face Inference API\n",
        "          inference_api = InferenceApi(\"huggingface/llama2b\")  # Replace with the correct model path if different\n",
        "\n",
        "          # Get response from Hugging Face\n",
        "          response = inference_api(prompts)\n",
        "          return response[\"generated_text\"]\n",
        "        elif model == \"llama2b\":\n",
        "          payload = {\"inputs\": prompts}\n",
        "          response = requests.post(API_URL, headers=headers, json=payload)\n",
        "          return response.json()[0][\"generated_text\"]\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Invalid model\")\n",
        "\n",
        "model = \"openai\"\n",
        "# model = \"llama2b\"  # Can be \"manual\", \"openai\", or \"huggingface\"\n",
        "\n",
        "\n",
        "def extract_probabilities(text):\n",
        "    # Use a regular expression to find patterns like \"(a) 20%\" or \"20%\"\n",
        "    pattern = r'\\b\\d+%|\\(\\w\\)\\s*\\d+%'\n",
        "    matches = re.findall(pattern, text)\n",
        "\n",
        "    # Process each match to convert to a decimal format\n",
        "    probabilities = []\n",
        "    for match in matches:\n",
        "        # Extracting just the numeric part\n",
        "        num = re.search(r'\\d+', match)\n",
        "        if num:\n",
        "            probability = int(num.group()) / 100\n",
        "            probabilities.append(probability)\n",
        "\n",
        "    # Convert the list to a NumPy array\n",
        "    probabilities_array = np.array(probabilities)\n",
        "\n",
        "    return probabilities_array\n",
        "\n",
        "\n",
        "\n",
        "def brier_score(true_values, predictions):\n",
        "    \"\"\"\n",
        "    Calculate the Brier score for a set of predictions.\n",
        "    true_values: array of actual values (1 for the true class, 0 for others)\n",
        "    predictions: array of predicted probabilities\n",
        "    \"\"\"\n",
        "    true_values = np.array(true_values)\n",
        "    predictions = np.array(predictions)\n",
        "    return np.mean((true_values - predictions) ** 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def calculate_answer_BTS(answer_vector, info_score_normalized, pred_score_normalized):\n",
        "    multiplied_pred = [a * b for a, b in zip(answer_vector, pred_score_normalized)]\n",
        "    multiplied_info = [a * b for a, b in zip(answer_vector, info_score_normalized)]\n",
        "    answer_info = [element / sum(multiplied_info) for element in multiplied_info]\n",
        "    answer_pred = [element / sum(multiplied_pred) for element in multiplied_pred]\n",
        "    return answer_info, answer_pred\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def calculate_scores(answer_vector, answer_pred_vector, number_of_choices):\n",
        "    # ic(answer_vector)\n",
        "    # ic(answer_pred_vector)\n",
        "    # ic(answer_pred_vector[0][0])\n",
        "    # ic(answer_pred_vector[3])\n",
        "    # Calculate information scores\n",
        "    info_score = []\n",
        "    for i in range(number_of_choices):\n",
        "        calc = answer_vector[i] * 1 / (answer_pred_vector[i][i]) if answer_pred_vector[i][i] != 0 else 0\n",
        "        info_score.append(calc)\n",
        "\n",
        "    # Calculate Brier scores for each prediction\n",
        "    pred_score = [brier_score(answer_vector, answer_pred_vector[i]) for i in range(number_of_choices)]\n",
        "\n",
        "    # Normalize prediction scores\n",
        "    total_pred_score = sum(pred_score)\n",
        "    pred_score_normalized = [score / total_pred_score if total_pred_score != 0 else 0 for score in pred_score]\n",
        "\n",
        "    # Normalize information scores\n",
        "    total_info_score = sum(info_score)\n",
        "    info_score_normalized = [score / total_info_score if total_info_score != 0 else 0 for score in info_score]\n",
        "\n",
        "    return info_score_normalized, pred_score_normalized\n",
        "\n",
        "def compare_vector_probabilities(vec1, vec2):\n",
        "    \"\"\"\n",
        "    Compares the position of highest probabilities in two vectors.\n",
        "\n",
        "    Parameters:\n",
        "    vec1 (list): The first vector of probabilities.\n",
        "    vec2 (list): The second vector of probabilities.\n",
        "\n",
        "    Returns:\n",
        "    int: 0 if the highest probabilities in both vectors are at the first position ,\n",
        "         1 if the highest probability in the first vector is at the first position but not in the second vector,\n",
        "        -1 if the highest probability in the second vector is at the first position but not in the first vector.\n",
        "        -2 neither are at the first position\n",
        "    \"\"\"\n",
        "    # Check if the highest probability in each vector is at the first position\n",
        "    is_first_max_vec1 = np.argmax(vec1) == 0\n",
        "    is_first_max_vec2 = np.argmax(vec2) == 0\n",
        "\n",
        "    # Determine the output based on the position of highest probabilities\n",
        "    if is_first_max_vec1 and is_first_max_vec2:\n",
        "        return 0\n",
        "    elif is_first_max_vec1 and not is_first_max_vec2:\n",
        "        return 1\n",
        "    elif not is_first_max_vec1 and is_first_max_vec2:\n",
        "        return -1\n",
        "    else:\n",
        "        return -2\n",
        "    # todo: seperate 0 into two parts\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKwlUSdarNrP"
      },
      "outputs": [],
      "source": [
        "def validate_answer_vector(answer_vector, answer, number_of_choices):\n",
        "    if len(answer_vector) != number_of_choices:\n",
        "        error_description = f\"Error: 'answer_vector' should have {number_of_choices} elements, but has {len(answer_vector)}.\"\n",
        "        return False, answer_vector, error_description\n",
        "    return True, answer_vector, \"Validation successful.\"\n",
        "\n",
        "\n",
        "def validate_answer_pred_vector(answer_pred_vector, answer_pred_list, number_of_choices):\n",
        "    if len(answer_pred_vector) != number_of_choices:\n",
        "        error_description = f\"Error: 'answer_pred_vector' should have {number_of_choices} sub-arrays, but has {len(answer_pred_vector)}.\"\n",
        "        return False, answer, error_description\n",
        "\n",
        "    for sub_array in answer_pred_vector:\n",
        "        if len(sub_array) != number_of_choices:\n",
        "            error_description = f\"Error: Each sub-array in 'answer_pred_vector' should have {number_of_choices} elements.\"\n",
        "            return False, answer_pred_vector, error_description\n",
        "\n",
        "    return True, answer_pred_list, \"Validation successful.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reorder_vectors(vec1, vec2, target):\n",
        "    \"\"\"\n",
        "    Reorders two vectors such that the element at the 'target' index in each vector is moved to the first position.\n",
        "    The rest of the elements maintain their order.\n",
        "\n",
        "    Parameters:\n",
        "    vec1 (list): The first vector.\n",
        "    vec2 (list): The second vector.\n",
        "    target (int): The index of the elements to move to the first position.\n",
        "\n",
        "    Returns:\n",
        "    tuple: The reordered vectors.\n",
        "    \"\"\"\n",
        "    # Ensure 'target' is a valid index for the vectors\n",
        "    if target < 0 or target >= len(vec1) or target >= len(vec2):\n",
        "        raise IndexError(\"Target index is out of range for the vectors.\")\n",
        "\n",
        "    # Reorder vec1\n",
        "    vec1 = [vec1[target]] + vec1[:target] + vec1[target + 1:]\n",
        "\n",
        "    # Reorder vec2\n",
        "    vec2 = [vec2[target]] + vec2[:target] + vec2[target + 1:]\n",
        "\n",
        "    return vec1, vec2\n",
        "\n",
        "\n",
        "def reorder_vector_and_vector_list(vec1, vec2_list, target):\n",
        "    \"\"\"\n",
        "    Reorders a numpy vector and a list of numpy vectors such that the element at the 'target' index\n",
        "    in the vector and each vector in the list is moved to the first position.\n",
        "    The rest of the elements maintain their order. The length of the vectors remains unchanged.\n",
        "\n",
        "    Parameters:\n",
        "    vec1 (numpy.ndarray): The vector.\n",
        "    vec2_list (list): The list of numpy vectors.\n",
        "    target (int): The index of the elements to move to the first position.\n",
        "\n",
        "    Returns:\n",
        "    tuple: The reordered vector and list of reordered vectors.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    # Check if vec1 is a numpy array\n",
        "    if not isinstance(vec1, np.ndarray):\n",
        "        raise TypeError(\"vec1 must be a numpy array.\")\n",
        "\n",
        "    # Check if vec2_list is a list of numpy arrays\n",
        "    if not all(isinstance(vec, np.ndarray) for vec in vec2_list):\n",
        "        raise TypeError(\"vec2_list must be a list of numpy arrays.\")\n",
        "\n",
        "    # Ensure 'target' is a valid index for vec1 and all vectors in vec2_list\n",
        "    if target < 0 or target >= len(vec1) or any(target >= len(vec) for vec in vec2_list):\n",
        "        raise IndexError(\"Target index is out of range for the vectors.\")\n",
        "\n",
        "    # Reorder vec1\n",
        "    vec1_reordered = np.concatenate(([vec1[target]], vec1[:target], vec1[target+1:]))\n",
        "\n",
        "    # Reorder each vector in vec2_list\n",
        "    vec2_list_reordered = [np.concatenate(([vec[target]], vec[:target], vec[target+1:])) for vec in vec2_list]\n",
        "\n",
        "    return vec1_reordered, vec2_list_reordered\n",
        "\n",
        "\n",
        "vec1 = np.array([1, 2, 3, 4])\n",
        "vec2_list = [np.array([10, 20, 30, 40]), np.array([50, 60, 70, 80])]\n",
        "target = 2\n",
        "\n",
        "reordered_vec1, reordered_vec2_list = reorder_vector_and_vector_list(vec1, vec2_list, target)\n",
        "print(\"Reordered vec1:\", reordered_vec1)\n",
        "print(\"Reordered vec2_list:\", reordered_vec2_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmEM2W1jste5",
        "outputId": "c47f93d3-6a69-4be8-8f5b-cd43ff0d1504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reordered vec1: [3 1 2 4]\n",
            "Reordered vec2_list: [array([30, 10, 20, 40]), array([70, 50, 60, 80])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6lVfSEvrBuO"
      },
      "outputs": [],
      "source": [
        "def process_question(question_idx, dataset, model, COT = 1):\n",
        "    # Extract the question and answer choices from the dataset\n",
        "    question = dataset[question_idx][\"question\"]\n",
        "    answer_choices = dataset[question_idx][\"mc1_targets\"][\"choices\"]\n",
        "    label = np.argmax(dataset[question_idx][\"mc1_targets\"][\"labels\"])\n",
        "    number_of_choices = len(answer_choices)\n",
        "\n",
        "    # Generate the main prompt\n",
        "    prompt = generate_prompt_answer(question, answer_choices, COT)\n",
        "\n",
        "    # Call the language model to get the main answer\n",
        "    output = call_llm(prompt, model)\n",
        "    answer = output.choices[0].message.content\n",
        "    answer_vector = extract_probabilities(answer)\n",
        "\n",
        "    # Generate predictor prompts and call the LLM for each\n",
        "    prompts_predictor = generate_prompts_pred(question, answer_choices, COT)\n",
        "    answer_pred_list = []\n",
        "    for i in range(number_of_choices):\n",
        "        output_pred = call_llm(prompts_predictor[i], model)\n",
        "        answer_pred = output_pred.choices[0].message.content\n",
        "        answer_pred_list.append(answer_pred)\n",
        "    # Convert predictions to vectors and calculate scores\n",
        "    answer_pred_vector = [extract_probabilities(pred) for pred in answer_pred_list]\n",
        "    # Assuming 'answer_vector' and 'answer_pred_vector' are already defined, along with 'number_of_choices'\n",
        "    is_valid, result, error_desc = validate_answer_vector(answer_vector, answer, number_of_choices)\n",
        "    if not is_valid:\n",
        "        print(error_desc, \"this concerns question\", question_idx)\n",
        "        print(\"answer_vector:\", result)\n",
        "        return None, None, None, None\n",
        "\n",
        "    is_valid, result, error_desc = validate_answer_pred_vector(answer_pred_vector, answer_pred_list, number_of_choices)\n",
        "    if not is_valid:\n",
        "        print(error_desc, \"this concerns question\", question_idx)\n",
        "        print(\"answer_pred_vector:\", result)\n",
        "        return None, None, None, None\n",
        "\n",
        "\n",
        "    #reorder:\n",
        "    info_score, pred_score = calculate_scores(answer_vector, answer_pred_vector, number_of_choices)\n",
        "\n",
        "    # Calculate the final results\n",
        "    answer_info, answer_pred = calculate_answer_BTS(answer_vector, info_score, pred_score)\n",
        "\n",
        "    LProbTruth = answer_vector[0]\n",
        "    LProbTruthwithInfo = answer_info[0]\n",
        "    LProbTruthwithPred = answer_pred[0]\n",
        "    switch = compare_vector_probabilities(answer_vector, answer_info)\n",
        "    return LProbTruth, LProbTruthwithInfo, LProbTruthwithPred, switch\n",
        "\n",
        "# Helper functions (generate_main_prompt, generate_prompts, call_llm, extract_probabilities, calculate_scores, calculate_answer_BTS) need to be defined separately.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_MMLU_question(question_idx, dataset, model, COT=1):\n",
        "    # Extract question and answer choices\n",
        "    question = dataset_MMLU[question_idx]['question']\n",
        "    answer_choices = dataset_MMLU[question_idx]['choices']\n",
        "    target = dataset_MMLU[question_idx]['answer']\n",
        "    number_of_choices = len(answer_choices)\n",
        "    ic(question)\n",
        "    ic(target)\n",
        "    # ic(number_of_choices)\n",
        "    ic(answer_choices)\n",
        "    # return None, None, None, None\n",
        "    # Generate the main prompt\n",
        "    prompt = generate_prompt_answer(question, answer_choices, COT)\n",
        "\n",
        "    # Call the language model to get the main answer\n",
        "    output = call_llm(prompt, model)\n",
        "    answer = output.choices[0].message.content\n",
        "    answer_vector = extract_probabilities(answer)\n",
        "    print(output)\n",
        "    # Generate predictor prompts and call the LLM for each\n",
        "    prompts_predictor = generate_prompts_pred(question, answer_choices, COT)\n",
        "    answer_pred_list = []\n",
        "    for i in range(number_of_choices):\n",
        "    output_pred = call_llm(prompts_predictor, model)\n",
        "    return output_pred\n",
        "    # answer_pred = output_pred.choices[0].message.content\n",
        "    # answer_pred_list.append(answer_pred)\n",
        "    print(answer_pred_list)\n",
        "    ic(answer_vector, \"first\")\n",
        "\n",
        "    # Convert predictions to vectors and calculate scores\n",
        "    answer_pred_vector = [extract_probabilities(pred) for pred in answer_pred_list]\n",
        "    # Assuming 'answer_vector' and 'answer_pred_vector' are already defined, along with 'number_of_choices'\n",
        "    is_valid, result, error_desc = validate_answer_vector(answer_vector, answer, number_of_choices)\n",
        "    if not is_valid:\n",
        "        print(error_desc, \"this concerns question\", question_idx)\n",
        "        print(\"answer_vector:\", result)\n",
        "        return None, None, None, None, None\n",
        "\n",
        "    is_valid, result, error_desc = validate_answer_pred_vector(answer_pred_vector, answer_pred_list, number_of_choices)\n",
        "    if not is_valid:\n",
        "        print(error_desc, \"this concerns question\", question_idx)\n",
        "        print(\"answer_pred_vector:\", result)\n",
        "        return None, None, None, None, None\n",
        "    # ic(answer_vector, \"second\")\n",
        "    # ic(answer_pred_vector, \"second\")\n",
        "    ic(type(target))\n",
        "    answer_vector, answer_pred_vector = reorder_vector_and_vector_list(answer_vector, answer_pred_vector, target)\n",
        "    ic(answer_vector, \"third\")\n",
        "\n",
        "    info_score, pred_score = calculate_scores(answer_vector, answer_pred_vector, number_of_choices)\n",
        "    ic(info_score)\n",
        "    ic(pred_score)\n",
        "    # Calculate the final results\n",
        "    answer_info, answer_pred = calculate_answer_BTS(answer_vector, info_score, pred_score)\n",
        "\n",
        "    LProbTruth = answer_vector[0]\n",
        "    LProbTruthwithInfo = answer_info[0]\n",
        "    LProbTruthwithPred = answer_pred[0]\n",
        "    switch = compare_vector_probabilities(answer_vector, answer_info)\n",
        "    return LProbTruth, LProbTruthwithInfo, LProbTruthwithPred, switch, answer_pred_vector\n"
      ],
      "metadata": {
        "id": "2L5agJuPnV9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**batching results**"
      ],
      "metadata": {
        "id": "SbCVwMb8xSB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*results* for TruthfulQA\n"
      ],
      "metadata": {
        "id": "O819sdTwu5HK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md2MqF2Srld8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d4ff9e2-8b01-43df-88f6-5655859bda32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1]\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| switch: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Each sub-array in 'answer_pred_vector' should have 4 elements. this concerns question 0\n",
            "answer_pred_vector: [array([0.8 , 0.1 , 0.05, 0.05]), array([0.1 , 0.8 , 0.05, 0.05]), array([], dtype=float64), array([0.1 , 0.8 , 0.05, 0.05])]\n",
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| switch: -2\n"
          ]
        }
      ],
      "source": [
        "results = {\"LProbTruth\": [], \"LProbTruthwithInfo\": [], \"LProbTruthwithPred\": [], \"switch\": []}\n",
        "\n",
        "\n",
        "question_indices = list(range(2))  # List of question indices from 0 to 20\n",
        "print(question_indices)\n",
        "for question_idx in question_indices:\n",
        "    print(question_idx)\n",
        "    LProbTruth, LProbTruthwithInfo, LProbTruthwithPred, switch = process_question(question_idx, dataset, model, COT = 0)\n",
        "    results[\"LProbTruth\"].append(LProbTruth)\n",
        "    results[\"LProbTruthwithInfo\"].append(LProbTruthwithInfo)\n",
        "    results[\"LProbTruthwithPred\"].append(LProbTruthwithPred)\n",
        "    results[\"switch\"].append(switch)\n",
        "    ic(switch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "results for MMLU\n"
      ],
      "metadata": {
        "id": "1fx3gGc9u9a1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_MMLU = {\"LProbTruth\": [], \"LProbTruthwithInfo\": [], \"LProbTruthwithPred\": [], \"switch\": []}\n",
        "\n",
        "\n",
        "question_indices = list(range(1, 2))  # List of question indices from 0 to 20\n",
        "print(question_indices)\n",
        "for question_idx in question_indices:\n",
        "    ic(question_idx)\n",
        "    test = process_MMLU_question(question_idx, dataset_MMLU, model, COT = 1)\n",
        "    results_MMLU[\"LProbTruth\"].append(LProbTruth)\n",
        "    results_MMLU[\"LProbTruthwithInfo\"].append(LProbTruthwithInfo)\n",
        "    results_MMLU[\"LProbTruthwithPred\"].append(LProbTruthwithPred)\n",
        "    results_MMLU[\"switch\"].append(switch)\n",
        "    ic(switch)\n",
        "\n",
        "\n",
        "ic(type(test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeEmuRY1BFM7",
        "outputId": "6ae2bd01-0c3b-420e-af9b-867b8a6cead5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| question_idx: 1\n",
            "ic| question: ('Which of the following is included in U.S. GDP? I.   The market value of '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               'movies made in Africa by U.S. citizens II.   The market value of olive oil '\n",
            "               'made in Italy and sold in the United States. III.   The market value of blue '\n",
            "               'jeans made in the United States and sold in Japan IV.   The market value of '\n",
            "               'wine made in the United States by Canadian citizens')\n",
            "ic| target: 3\n",
            "ic| answer_choices: ['II III and IV only', 'I and III only', 'II and IV only', 'III and IV only']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8RqSbcPpsNGONvtn1Pps3kHuOhkvG', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=\"The market value of goods and services produced within a country's borders is included in GDP. Therefore, option (d) III and IV only is most likely to be included in U.S. GDP.\\n\\n(a) 0%\\n(b) 0%\\n(c) 0%\\n(d) 100%\", role='assistant', function_call=None, tool_calls=None))], created=1701647205, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_eeff13170a', usage=CompletionUsage(completion_tokens=60, prompt_tokens=215, total_tokens=275))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| answer_vector: array([0., 0., 0., 1.]), 'first'\n",
            "ic"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The market value of olive oil made in Italy and sold in the United States (II) and the market value of blue jeans made in the United States and sold in Japan (III) are included in U.S. GDP, but the market value of movies made in Africa by U.S. citizens (I) and the market value of wine made in the United States by Canadian citizens (IV) are not.\\n\\nGiven the question and the options, I predict the following probability distribution over answers:\\n(a) 40%\\n(b) 10%\\n(c) 20%\\n(d) 30%', 'The correct options are I and III, as they both involve market value generated within the United States. It is unlikely that many people would choose options II, as it involves a product made in Italy, or IV, as it involves Canadian citizens making the product. Therefore, my predictions are:\\n\\n(a) 10%\\n(b) 70%\\n(c) 10%\\n(d) 10%', 'The market value of olive oil made in Italy and sold in the United States (Option II) would be included in U.S. GDP as it represents a good produced within the U.S. economy. The market value of wine made in the United States by Canadian citizens (Option IV) would also be included in U.S. GDP for the same reason.\\n\\nPredicted probability distribution:\\n(a) 20%\\n(b) 10%\\n(c) 60%\\n(d) 10%', 'Based on my understanding of U.S. GDP components, only the market value of goods produced within the U.S. (III) and not made in the U.S. but by U.S. citizens (IV) would be included in the GDP. The market value of goods made outside the U.S. and sold in the U.S. (II) would not be included. Therefore, I predict the following probability distribution:\\n\\n(a) 0%\\n(b) 0%\\n(c) 0%\\n(d) 100%']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "| type(target): <class 'int'>\n",
            "ic| answer_vector: array([1., 0., 0., 0.]), 'third'\n",
            "ic| info_score: [1.0, 0.0, 0.0, 0.0]\n",
            "ic| pred_score: [0.21604938271604934, 0.4074074074074074, 0.37654320987654327, 0.0]\n",
            "ic| switch: -2\n",
            "ic| type(test): <class 'tuple'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tuple"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ic(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYLsunmDE_o2",
        "outputId": "377ee4a4-a412-4f6c-cfa9-da87978a0591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| test: (1.0,\n",
            "           1.0,\n",
            "           1.0,\n",
            "           0,\n",
            "           [array([0.3, 0.4, 0.1, 0.2]),\n",
            "            array([0.1, 0.1, 0.7, 0.1]),\n",
            "            array([0.1, 0.2, 0.1, 0.6]),\n",
            "            array([1., 0., 0., 0.])])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0,\n",
              " [array([0.3, 0.4, 0.1, 0.2]),\n",
              "  array([0.1, 0.1, 0.7, 0.1]),\n",
              "  array([0.1, 0.2, 0.1, 0.6]),\n",
              "  array([1., 0., 0., 0.])])"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_MMLU = {\"LProbTruth\": [], \"LProbTruthwithInfo\": [], \"LProbTruthwithPred\": [], \"switch\": []}\n",
        "\n",
        "\n",
        "question_indices = list(range(40, 70))  # List of question indices from 0 to 20\n",
        "print(question_indices)\n",
        "for question_idx in question_indices:\n",
        "    ic(question_idx)\n",
        "    LProbTruth, LProbTruthwithInfo, LProbTruthwithPred, switch, answer_pred_vector = process_MMLU_question(question_idx, dataset_MMLU, model, COT = 1)\n",
        "    results_MMLU[\"LProbTruth\"].append(LProbTruth)\n",
        "    results_MMLU[\"LProbTruthwithInfo\"].append(LProbTruthwithInfo)\n",
        "    results_MMLU[\"LProbTruthwithPred\"].append(LProbTruthwithPred)\n",
        "    results_MMLU[\"switch\"].append(switch)\n",
        "    ic(switch)\n"
      ],
      "metadata": {
        "id": "IsBJMhpxu-84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3604f80c-75db-4251-a9b4-0450e28ffd66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| question_idx: 40\n",
            "ic| question: ('Which of the following policies is most likely to bring about economic '\n",
            "               'growth in the long run?')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| target: 3\n",
            "ic| answer_choices: ['Imposing tariffs to protect domestic industries from foreign competition.',\n",
            "                     'Placing taxes on savings.',\n",
            "                     'Increasing government spending.',\n",
            "                     'Promoting improvements in the education of the population.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8RqRjR3zBBznYIn8ZVYLbe8QChUlu', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Promoting improvements in the education of the population is most likely to bring about economic growth in the long run because a well-educated population can lead to innovation and productivity gains.\\n\\n(a) 10%\\n(b) 5%\\n(c) 5%\\n(d) 80%', role='assistant', function_call=None, tool_calls=None))], created=1701647151, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_eeff13170a', usage=CompletionUsage(completion_tokens=54, prompt_tokens=161, total_tokens=215))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| answer_vector: array([0.1 , 0.05, 0.05, 0.8 ]), 'first'\n",
            "ic| type(target): <class 'int'>\n",
            "ic| answer_vector: array(["
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['First, I believe that some people may support imposing tariffs as a way to protect domestic industries and promote economic growth.\\nSecond, I think a smaller percentage of people might choose increasing government spending as a policy for economic growth.\\nFinally, I predict very few people would choose placing taxes on savings or promoting improvements in education as the most likely policies for economic growth.\\nSo my probability distribution would be:\\n(a) 60%\\n(b) 20%\\n(c) 15%\\n(d) 5%', \"The policy of imposing tariffs can lead to retaliatory tariffs and trade wars, which can harm economic growth. Placing taxes on savings can potentially reduce incentives to save and invest, leading to lower economic growth.\\n\\nNow, my predictions for others' answers:\\n(a) 15%\\n(b) 20%\\n(c) 10%\\n(d) 55%\", 'Increasing government spending can stimulate economic activity and create demand for goods and services, which can ultimately lead to long-term economic growth.\\n\\nPredicted probability distribution:\\n(a) 10%\\n(b) 5%\\n(c) 70%\\n(d) 15%', 'Imposing tariffs and placing taxes on savings are likely to hinder economic growth by reducing efficiency and discouraging investment and savings. Increasing government spending can stimulate short-term growth, but may also lead to higher debt and inflation in the long run. \\n\\nTherefore, promoting improvements in the education of the population is most likely to bring about economic growth in the long run as it can lead to better skilled and innovative workforce, increased productivity, and technological advancements.\\n\\nSo, my predicted probability distribution over answers that others would give is:\\n(a) 10%\\n(b) 5%\\n(c) 15%\\n(d) 70%']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.8 , 0.1 , 0.05, 0.05]), 'third'\n",
            "ic| info_score: [0.8888888888888888,\n",
            "                 0.03703703703703704,\n",
            "                 0.05555555555555555,\n",
            "                 0.01851851851851852]\n",
            "ic| pred_score: [0.4694444444444445,\n",
            "                 0.05000000000000001,\n",
            "                 0.4694444444444445,\n",
            "                 0.011111111111111122]\n",
            "ic| switch: 0\n",
            "ic| question_idx: 41\n",
            "ic| question: 'When a country has a balance of trade deficit'\n",
            "ic| target: 3\n",
            "ic| answer_choices: ['it must make up the difference by shipping gold to its creditors.',\n",
            "                     'its exports exceed its imports.',\n",
            "                     'its currency will appreciate.',\n",
            "                     'its imports exceed its exports.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8RqRp1gOng4GSXWjIOHAdaBDT9e8Y', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=\"I think the most likely answer is (d) its imports exceed its exports, as a balance of trade deficit means that a country is importing more than it is exporting. Option (a) is highly unlikely, as shipping gold to cover the deficit is a rare occurrence in modern economies. Option (b) is also unlikely as a balance of trade deficit specifically indicates the opposite. Option (c) is also unlikely as a deficit usually puts downward pressure on a country's currency, not an appreciation. \\n\\nSo, I would distribute the probabilities as follows:\\n\\n(a) 5%\\n(b) 10%\\n(c) 5%\\n(d) 80%\", role='assistant', function_call=None, tool_calls=None))], created=1701647157, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_eeff13170a', usage=CompletionUsage(completion_tokens=130, prompt_tokens=152, total_tokens=282))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| answer_vector: array([0.05, 0.1 , 0.05, 0.8 ]), 'first'\n",
            "ic| type(target): <class 'int'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"The correct answer is d, as a trade deficit means that a country's imports exceed its exports. Therefore, the country has to make up the difference by importing more than it is exporting.\\n\\nNow, let me calculate the predicted probability distribution. \\n\\n(a) 10%\\n(b) 10%\\n(c) 10%\\n(d) 70%\", 'The correct answer is (d) its imports exceed its exports because a balance of trade deficit means a country is importing more than it is exporting.\\nI predict:\\n(a) 10%\\n(b) 20%\\n(c) 10%\\n(d) 60%', \"The answer is (d) its imports exceed its exports because a trade deficit occurs when a country's imports exceed its exports.\\n\\nPrediction:\\n(a) 10%\\n(b) 5%\\n(c) 15%\\n(d) 70%\", 'Sure, here are my predictions:\\n\\n(a) 20%\\n(b) 10%\\n(c) 5%\\n(d) 65%\\n\\nI predict that most people would choose (d) because it is the correct answer that reflects a balance of trade deficit. Some might choose (a) due to historical references to shipping gold to creditors. Fewer might choose (b) or (c) due to misunderstandings of trade deficit dynamics.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| answer_vector: array([0.8 , 0.05, 0.1 , 0.05]), 'third'\n",
            "ic| info_score: [0.24615384615384617,\n",
            "                 0.10769230769230768,\n",
            "                 0.43076923076923074,\n",
            "                 0.21538461538461537]\n",
            "ic| pred_score: [0.10714285714285722,\n",
            "                 0.3928571428571429,\n",
            "                 0.1785714285714286,\n",
            "                 0.32142857142857134]\n",
            "ic| switch: 0\n",
            "ic| question_idx: 42\n",
            "ic| question: 'National income measures'\n",
            "ic| target: 3\n",
            "ic| answer_choices: ['household income in the nation.',\n",
            "                     'income earned by the factors of production.',\n",
            "                     'GDP minus depreciation and indirect business taxes.',\n",
            "                     '(B) (C) and (D).']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8RqRwZn4bQ6mFgfgNQ9v34M0w9DOa', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='The national income measures the total income earned by the factors of production in a nation, which includes both GDP and indirect business taxes. It does not include household income.\\n\\n(a) 10%\\n(b) 70%\\n(c) 20%\\n(d) 0%', role='assistant', function_call=None, tool_calls=None))], created=1701647164, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_eeff13170a', usage=CompletionUsage(completion_tokens=53, prompt_tokens=148, total_tokens=201))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| answer_vector: array([0.1, 0.7, 0.2, 0. ]), 'first'\n",
            "ic| type(target): <class 'int'>\n",
            "ic"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['First, I know that national income measures include household income, so I predict that most people would choose option (a) with a high probability. Conversely, options (b) and (c) are less relevant to national income, so I predict lower probabilities for those options. Therefore, my predicted probability distribution would be:\\n(a) 60%\\n(b) 20%\\n(c) 10%\\n(d) 10%', 'Based on my understanding of the question, national income measures the income earned by the factors of production. Therefore, I predict that most people would also choose option (b) income earned by the factors of production.\\n\\nPlease provide your predictions in this format.\\n(a) 10%\\n(b) 85%\\n(c) 5%\\n(d) 0%', 'The answer c, \"GDP minus depreciation and indirect business taxes\", represents the measure of national income, so I predict that most people would choose this option.\\n\\nPlease provide your predictions in this format.\\n(a) 10%\\n(b) 15%\\n(c) 70%\\n(d) 5%', 'The national income is more accurately measured by the income earned by the factors of production (b), GDP minus depreciation and indirect business taxes (c), and household income in the nation (d). Therefore, I predict the following distribution: \\n\\n(a) 10%\\n(b) 20%\\n(c) 30%\\n(d) 40%']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "| answer_vector: array([0. , 0.1, 0.7, 0.2]), 'third'\n",
            "ic| info_score: [0.0, 0.15789473684210525, 0.7368421052631579, 0.10526315789473685]\n",
            "ic| pred_score: [0.33766233766233766,\n",
            "                 0.029220779220779227,\n",
            "                 0.3603896103896103,\n",
            "                 0.2727272727272727]\n",
            "ic| switch: -2\n",
            "ic| question_idx: 43\n",
            "ic| question: ('Assume the reserve requirement is 10 percent. If the FED sells $29 million '\n",
            "               'worth of government securities in an open market operation then the money '\n",
            "               'supply can')\n",
            "ic| target: 3\n",
            "ic| answer_choices: ['increase by $2.9 million.',\n",
            "                     'decrease by $2.9 million.',\n",
            "                     'increase by $290 million.',\n",
            "                     'decrease by $290 million.']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-179-a94d1bf80054>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquestion_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestion_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mLProbTruth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLProbTruthwithInfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLProbTruthwithPred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_pred_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_MMLU_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_MMLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mresults_MMLU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"LProbTruth\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLProbTruth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mresults_MMLU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"LProbTruthwithInfo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLProbTruthwithInfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-173-6aeb07a9c295>\u001b[0m in \u001b[0;36mprocess_MMLU_question\u001b[0;34m(question_idx, dataset, model, COT)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Call the language model to get the main answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0manswer_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-169-86cd8f804644>\u001b[0m in \u001b[0;36mcall_llm\u001b[0;34m(prompt, model)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"openai\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             response = openai.chat.completions.create(\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo-1106\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# or another model of your choice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;31m# model=\"gpt-4\",  # or another model of your choice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    599\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         )\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 856\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             response = self._client.send(\n\u001b[0m\u001b[1;32m    883\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m                 \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_auth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    902\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    930\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    964\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    226\u001b[0m         )\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mShieldCancellation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionNotAvailable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0;31m# The ConnectionNotAvailable exception is a special case, that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mConnectionNotAvailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mreason_phrase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    112\u001b[0m                 trace.return_value = (\n\u001b[1;32m    113\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    213\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1286\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_questions_batch(question_indices, dataset, model, COT=1):\n",
        "    results = {\"LProbTruth\": [], \"LProbTruthwithInfo\": [], \"LProbTruthwithPred\": [], \"switch\": []}\n",
        "\n",
        "    for question_idx in question_indices:\n",
        "        question = dataset[question_idx][\"question\"]\n",
        "        answer_choices = dataset[question_idx][\"mc1_targets\"][\"choices\"]\n",
        "        number_of_choices = len(answer_choices)\n",
        "\n",
        "        # Generate the main prompt and predictor prompts\n",
        "        main_prompt = generate_prompt_answer(question, answer_choices, COT)\n",
        "        predictor_prompts = generate_prompts_pred(question, answer_choices, COT)\n",
        "\n",
        "        # Call the LLM in batch for main and predictor prompts\n",
        "        main_output = call_llm([main_prompt], model)[0]\n",
        "\n",
        "        # Validation and score calculation (assuming relevant functions are defined)\n",
        "        # ... [Insert the validation and score calculation logic here] ...\n",
        "\n",
        "        # Calculate the final results\n",
        "        answer_info, answer_pred = calculate_answer_BTS(answer_vector, info_score, pred_score)\n",
        "\n",
        "        LProbTruth = answer_vector[0]\n",
        "        LProbTruthwithInfo = answer_info[0]\n",
        "        LProbTruthwithPred = answer_pred[0]\n",
        "        switch = compare_vector_probabilities(answer_vector, answer_info)\n",
        "\n",
        "        results[\"LProbTruth\"].append(LProbTruth)\n",
        "        results[\"LProbTruthwithInfo\"].append(LProbTruthwithInfo)\n",
        "        results[\"LProbTruthwithPred\"].append(LProbTruthwithPred)\n",
        "        results[\"switch\"].append(switch)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example usage\n",
        "question_indices = list(range(7))  # List of question indices from 0 to 6\n",
        "print(dataset)\n",
        "print(model)\n",
        "results = process_questions_batch(question_indices, dataset, model, COT=0)\n"
      ],
      "metadata": {
        "id": "xk17RC4GAJ6g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "4fa68b26-b6c3-4731-ba18-b43675c8bfa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['question', 'mc1_targets', 'mc2_targets'],\n",
            "    num_rows: 817\n",
            "})\n",
            "openai\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-180-e47c433c36ce>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_questions_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-180-e47c433c36ce>\u001b[0m in \u001b[0;36mprocess_questions_batch\u001b[0;34m(question_indices, dataset, model, COT)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Call the LLM in batch for main and predictor prompts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mmain_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmain_prompt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Validation and score calculation (assuming relevant functions are defined)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-169-86cd8f804644>\u001b[0m in \u001b[0;36mcall_llm\u001b[0;34m(prompt, model)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"openai\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             response = openai.chat.completions.create(\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo-1106\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# or another model of your choice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;31m# model=\"gpt-4\",  # or another model of your choice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    599\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         )\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 856\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"'$.messages[0].content' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM6Jjj46s9wa"
      },
      "source": [
        "Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ic(results_MMLU[\"LProbTruth\"])\n",
        "ic(results_MMLU[\"LProbTruthwithInfo\"])\n",
        "ic(results_MMLU[\"LProbTruthwithPred\"])\n",
        "ic(results_MMLU[\"switch\"])\n",
        "k = answer_pred_vector\n",
        "print(k)\n",
        "\n",
        "results = results_MMLU"
      ],
      "metadata": {
        "id": "GSv6qwwsJgMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAWs4fGTs9Rl"
      },
      "outputs": [],
      "source": [
        "\n",
        "ic(results[\"LProbTruth\"])\n",
        "ic(results[\"LProbTruthwithInfo\"])\n",
        "ic(results[\"LProbTruthwithPred\"])\n",
        "ic(results[\"switch\"])\n",
        "k = answer_pred_vector\n",
        "print(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBZLkgXxv5f8"
      },
      "outputs": [],
      "source": [
        "results_GPT4_COT1 = {\"LProbTruth\": [], \"LProbTruthwithInfo\": [], \"LProbTruthwithPred\": []}\n",
        "\n",
        "results_GPT4_COT1[\"LProbTruth\"] = [0.0,\n",
        "                            0.8,\n",
        "                            0.9,\n",
        "                            0.6,\n",
        "                            0.95,\n",
        "                            0.9,\n",
        "                            1.0,\n",
        "                            None,\n",
        "                            1.0,\n",
        "                            0.7,\n",
        "                            0.5,\n",
        "                            1.0,\n",
        "                            1.0,\n",
        "                            0.8,\n",
        "                            1.0,\n",
        "                            0.6,\n",
        "                            None,\n",
        "                            0.6,\n",
        "                            0.97]\n",
        "results_GPT4_COT1[\"LProbTruthwithInfo\"] = [0.0,\n",
        "                                    0.9388753056234718,\n",
        "                                    0.8925619834710744,\n",
        "                                    0.6749999999999999,\n",
        "                                    0.9954776086476947,\n",
        "                                    0.9759036144578314,\n",
        "                                    1.0,\n",
        "                                    None,\n",
        "                                    1.0,\n",
        "                                    0.8391009632536566,\n",
        "                                    0.6177606177606177,\n",
        "                                    1.0,\n",
        "                                    1.0,\n",
        "                                    0.9739130434782608,\n",
        "                                    1.0,\n",
        "                                    0.8083832335329341,\n",
        "                                    None,\n",
        "                                    0.7780419278149988,\n",
        "                                    0.9991929146401036]\n",
        "results_GPT4_COT1[\"LProbTruthwithPred\"] = [0.0,\n",
        "                                    0.9289263339814215,\n",
        "                                    0.23517587939698495,\n",
        "                                    0.4137931034482759,\n",
        "                                    0.92455637606084,\n",
        "                                    0.8292295472597297,\n",
        "                                    1.0,\n",
        "                                    None,\n",
        "                                    1.0,\n",
        "                                    0.3046209062359801,\n",
        "                                    0.05291005291005291,\n",
        "                                    1.0,\n",
        "                                    1.0,\n",
        "                                    0.8,\n",
        "                                    1.0,\n",
        "                                    0.40206185567010294,\n",
        "                                    None,\n",
        "                                    0.2588850195540813,\n",
        "                                    0.9695166915794717]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWci42IH0PZo"
      },
      "outputs": [],
      "source": [
        "results_GPT4_COT0 = {\n",
        "    \"LProbTruth\": [None, 0.7, 0.8, 0.8, 0.95, 0.9, None, None, None, 0.6, 0.4, 1.0, 1.0, 0.2, None, 0.3, 0.25, 0.4, 0.8, 0.4, 1.0],\n",
        "    \"LProbTruthwithInfo\": [None, 0.7921131590227175, 0.7710843373493976, 0.8886317616430431, 0.9966869133075651, 0.9932962720732504, None, None, None, 0.7815468113975577, 0.4526166902404527, 1.0, 1.0, 0.12500000000000003, None, 0.39669421487603307, 1.0, 0.4876190476190476, 0.9642910953744159, 0.2706131078224102, 1.0],\n",
        "    \"LProbTruthwithPred\": [None, 0.0, 0.17582417582417584, 0.5045045045045047, 0.9221420912949003, 0.8885730528709495, None, None, None, 0.5781042996507286, 0.3478260869565217, 1.0, 1.0, 1.0, None, 0.21052631578947362, 1.0, 0.14312977099236646, 0.6975156621300497, 0.2406015037593984, 1.0]\n",
        "}\n",
        "\n",
        "# results = results_GPT4_COT0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jH58BPgc6fsD"
      },
      "outputs": [],
      "source": [
        "results_GPT35_COT3 = {\n",
        "    \"LProbTruth\": [0.1, 0.8, 0.95, 0.8, 0.95, 0.8, 0.99, 0.2, 0.99, 0.6, 0.7, 1.0, 1.0, 0.3, 0.0, 0.4, 0.15, 0.4, 0.8, 0.9, 1.0, None, 0.8, 1.0, None, None, 0.4, 0.3, 0.9, 0.75, 0.9, 0.0, None, 0.9, 0.9, 0.99, 0.7],\n",
        "    \"LProbTruthwithInfo\": [0.013913043478260872, 0.8614694335389793, 0.9846976350890595, 0.9431578947368421, 0.9166158846231971, 0.9770681594096878, 0.368041256469793, 0.17438692098092645, 0.9919036534763688, 0.8239871824216067, 0.8860759493670886, 1.0, 1.0, 0.295687885010267, None, 0.4866920152091256, 0.04326923076923077, 0.4446650124069479, 0.919650341276494, 0.976374156219865, 1.0, None, 0.4067796610169492, 1.0, None, None, 0.4477611940298507, 0.04986149584487535, 0.981770580820324, 0.87890625, 0.9391304347826087, 0.0, None, 0.9540636042402826, 0.9860512300278975, 0.9985736118186449, 0.896341463414634],\n",
        "    \"LProbTruthwithPred\": [0.17875, 0.6823997725334091, 0.891535015326574, 0.5042979942693411, 0.7349485087197279, 0.6968403074295475, 0.43832344970080767, 0.12963138444044345, 0.9946195600419009, 0.579951530349674, 0.14237288135593215, 1.0, 1.0, 0.0967741935483869, None, 0.12631578947368413, 0.19534883720930232, 0.40536298820680694, 0.5362162162162161, 0.7926341072858287, 1.0, None, 0.8460236886632826, 1.0, None, None, 0.27350427350427353, 0.3901840490797547, 0.9110903558193487, 0.47806225432911936, 0.9295774647887324, 0.0, None, 0.9282103134479273, 0.786114221724524, 0.9859904270986745, 0.784660766961652]\n",
        "}\n",
        "# results = results_GPT35_COT3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKQe-HdB6aHu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_vectors(vector1, vector2, title='Vector Comparison', vector1_name='Vector 1', vector2_name='Vector 2'):\n",
        "    \"\"\"\n",
        "    Plots two vectors against each other as dots, ignoring 'None' values, and includes a 45-degree line.\n",
        "    Allows customization of the plot title and axis labels.\n",
        "\n",
        "    :param vector1: List of integers or 'None', representing the first vector.\n",
        "    :param vector2: List of integers or 'None', representing the second vector.\n",
        "    :param title: Title of the plot.\n",
        "    :param vector1_name: Label for the x-axis.\n",
        "    :param vector2_name: Label for the y-axis.\n",
        "    \"\"\"\n",
        "    # Filter out pairs where either value is None\n",
        "    filtered_pairs = [(x, y) for x, y in zip(vector1, vector2) if x is not None and y is not None]\n",
        "\n",
        "    # Unzip the pairs back into two lists\n",
        "    filtered_vector1, filtered_vector2 = zip(*filtered_pairs) if filtered_pairs else ([], [])\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(filtered_vector1, filtered_vector2, marker='o', color='b', s=50)\n",
        "    plt.plot([0, 1], [0, 1], color='red', linestyle='-', linewidth=1)  # 45-degree line\n",
        "    plt.title(title)\n",
        "    plt.xlabel(vector1_name)\n",
        "    plt.ylabel(vector2_name)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3zOLbY46hMA"
      },
      "outputs": [],
      "source": [
        "plot_vectors(results[\"LProbTruthwithInfo\"], results[\"LProbTruth\"], title='Info-weighted vs. Ususal', vector1_name='Prob(Truth with Info-Weighting)', vector2_name='Pr(Truth Default)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rb_jN9iS7d8t"
      },
      "outputs": [],
      "source": [
        "plot_vectors(results[\"LProbTruthwithInfo\"], results[\"LProbTruthwithPred\"], title='Info-weighted vs. Pred-weighted', vector1_name='Prob(Truth with Info-Weighting)', vector2_name='Pr(Truth with Pred-Weighting)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERvlQOOe_EOt"
      },
      "outputs": [],
      "source": [
        "plot_vectors(results[\"LProbTruthwithPred\"], results[\"LProbTruth\"], title='Pred-weighted vs. Ususal', vector1_name='Prob(Truth with Pred-Weighting)', vector2_name='Pr(Truth Default)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnjnQ9wf7fvW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_indexed_vectors(vector1, vector2, title='Indexed Vector Comparison', vector1_name='Vector 1', vector2_name='Vector 2'):\n",
        "    \"\"\"\n",
        "    Plots two vectors against their indices, with each vector's values represented in different colors.\n",
        "    Filters out positions where either vector has None, then sorts by the values of vector1 in descending order.\n",
        "    Allows customization of the plot title and vector names.\n",
        "\n",
        "    :param vector1: List of integers or None, representing the first vector.\n",
        "    :param vector2: List of integers or None, representing the second vector.\n",
        "    :param title: Title of the plot.\n",
        "    :param vector1_name: Name for the first vector (used in the legend).\n",
        "    :param vector2_name: Name for the second vector (used in the legend).\n",
        "    \"\"\"\n",
        "    # Filter out positions where either vector has None\n",
        "    filtered_vector1 = []\n",
        "    filtered_vector2 = []\n",
        "    for v1, v2 in zip(vector1, vector2):\n",
        "        if v1 is not None and v2 is not None:\n",
        "            filtered_vector1.append(v1)\n",
        "            filtered_vector2.append(v2)\n",
        "\n",
        "    # Sort filtered vectors based on the values of filtered_vector1\n",
        "    sorted_indices = sorted(range(len(filtered_vector1)), key=lambda i: filtered_vector1[i], reverse=True)\n",
        "    sorted_vector1 = [filtered_vector1[i] for i in sorted_indices]\n",
        "    sorted_vector2 = [filtered_vector2[i] for i in sorted_indices]\n",
        "\n",
        "    # Indices for plotting\n",
        "    indices = range(len(sorted_vector1))\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(indices, sorted_vector1, color='red', label=vector1_name)\n",
        "    plt.scatter(indices, sorted_vector2, color='blue', label=vector2_name)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Sorted Index (based on Vector 1 values)')\n",
        "    plt.ylabel('Values')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD2YBx5G7jGZ"
      },
      "outputs": [],
      "source": [
        "plot_indexed_vectors(results[\"LProbTruthwithInfo\"], results[\"LProbTruth\"],  title='Info-weighted vs. Ususal', vector1_name='Prob(Truth with Info-Weighting)', vector2_name='Pr(Truth Default)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xefLfC2e7vGb"
      },
      "source": [
        "Summary Statistics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tjygwj-I7uwJ"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import ttest_rel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Filtering out None values and keeping only the corresponding pairs\n",
        "filtered_prob_truth = [p for p, q in zip(results[\"LProbTruth\"], results[\"LProbTruthwithInfo\"]) if p is not None and q is not None]\n",
        "filtered_prob_truth_with_info = [q for p, q in zip(results[\"LProbTruth\"], results[\"LProbTruthwithInfo\"]) if p is not None and q is not None]\n",
        "\n",
        "# Performing a Paired T-Test\n",
        "t_statistic, p_value = ttest_rel(filtered_prob_truth, filtered_prob_truth_with_info)\n",
        "\n",
        "# Calculating the means\n",
        "mean_prob_truth = np.mean(filtered_prob_truth)\n",
        "mean_prob_truth_with_info = np.mean(filtered_prob_truth_with_info)\n",
        "\n",
        "# Creating a table for the means\n",
        "mean_table = pd.DataFrame({\n",
        "    \"Group\": [\"Probability(Truth)\", \"Probability(Truth with Info)\"],\n",
        "    \"Mean\": [mean_prob_truth, mean_prob_truth_with_info]\n",
        "})\n",
        "\n",
        "ic(t_statistic)\n",
        "ic(p_value)\n",
        "\n",
        "\n",
        "mean_table\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv5BOAV43m05"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def summarize_switches(switch_vector):\n",
        "    \"\"\"\n",
        "    Summarizes the occurrences of switch outcomes in a vector.\n",
        "\n",
        "    Parameters:\n",
        "    switch_vector (list): A list of integers representing switch outcomes.\n",
        "\n",
        "    Returns:\n",
        "    A pandas DataFrame: A table with the counts of each outcome and their explanations.\n",
        "    \"\"\"\n",
        "    # Define outcome explanations\n",
        "    outcomes = {\n",
        "        \"Stay at truth\": 0,\n",
        "        \"Switch away from truth\": 1,\n",
        "        \"Switch towards truth\": -1,\n",
        "        \"Never at truth\": -2\n",
        "    }\n",
        "\n",
        "    # Count the occurrences\n",
        "    counts = {explanation: switch_vector.count(outcome) for explanation, outcome in outcomes.items()}\n",
        "\n",
        "    # Create a DataFrame for display\n",
        "    summary_table = pd.DataFrame(list(counts.items()), columns=['Outcome', 'Count'])\n",
        "    summary_table = summary_table.set_index('Outcome')  # Set the 'Outcome' column as the index for a cleaner look\n",
        "\n",
        "    return summary_table\n",
        "\n",
        "# Example usage (assuming `ic` is a function and `results` is a dictionary containing \"switch\" as a key)\n",
        "# Replace ic(results[\"switch\"]) with an actual list of integers containing 0, 1, -1, and -2 for the real use case.\n",
        "switch_vector_example = [1, -1, 0, -2, 1, 0, -1, -2]\n",
        "summary_table = summarize_switches(switch_vector_example)\n",
        "print(summary_table)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "summary_table = summarize_switches(ic(results[\"switch\"]))\n",
        "print(summary_table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUg6C67xChB4"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhA6YZ6wCgw9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In-Context Learning SETUP**\n",
        "\n",
        "\n",
        "\n",
        "This should replace the above function. It is just an extension.\n",
        "It should also change for the Truthful QA one. I should have one function that handls these different datasets and improvement_round or not.\n",
        "\n"
      ],
      "metadata": {
        "id": "kZlEZuLURW5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_MMLU_question(question_idx, dataset, model, COT=1, improvement_round = False):\n",
        "    # Extract question and answer choices\n",
        "    question = dataset_MMLU[question_idx]['question']\n",
        "    answer_choices = dataset_MMLU[question_idx]['choices']\n",
        "    target = dataset_MMLU[question_idx]['answer']\n",
        "    number_of_choices = len(answer_choices)\n",
        "    ic(question)\n",
        "    ic(target)\n",
        "    ic(answer_choices)\n",
        "\n",
        "    # Generate the main prompt\n",
        "    prompt = generate_prompt_answer(question, answer_choices, COT)\n",
        "\n",
        "    # Call the language model to get the main answer\n",
        "    output = call_llm(prompt, model)\n",
        "    answer = output.choices[0].message.content\n",
        "    answer_vector = extract_probabilities(answer)\n",
        "    print(output)\n",
        "    # Generate predictor prompts and call the LLM for each\n",
        "    prompts_predictor = generate_prompts_pred(question, answer_choices, COT)\n",
        "    answer_pred_list = []\n",
        "    for i in range(number_of_choices):\n",
        "        output_pred = call_llm(prompts_predictor[i], model)\n",
        "        answer_pred = output_pred.choices[0].message.content\n",
        "        answer_pred_list.append(answer_pred)\n",
        "    print(answer_pred_list)\n",
        "    ic(answer_vector, \"first\")\n",
        "\n",
        "    # Convert predictions to vectors and calculate scores\n",
        "    answer_pred_vector = [extract_probabilities(pred) for pred in answer_pred_list]\n",
        "    # Assuming 'answer_vector' and 'answer_pred_vector' are already defined, along with 'number_of_choices'\n",
        "    is_valid, result, error_desc = validate_answer_vector(answer_vector, answer, number_of_choices)\n",
        "    if not is_valid:\n",
        "        print(error_desc, \"this concerns question\", question_idx)\n",
        "        print(\"answer_vector:\", result)\n",
        "        return None, None, None\n",
        "\n",
        "    is_valid, result, error_desc = validate_answer_pred_vector(answer_pred_vector, answer_pred_list, number_of_choices)\n",
        "    if not is_valid:\n",
        "        print(error_desc, \"this concerns question\", question_idx)\n",
        "        print(\"answer_pred_vector:\", result)\n",
        "        return None, None, None\n",
        "    ic(type(target))\n",
        "    answer_vector_reordered, answer_pred_vector_reordered = reorder_vector_and_vector_list(answer_vector, answer_pred_vector, target)\n",
        "    ic(answer_vector_reordered, \"third\")\n",
        "\n",
        "    info_score, pred_score = calculate_scores(answer_vector_reordered, answer_pred_vector_reordered, number_of_choices)\n",
        "    ic(info_score)\n",
        "    ic(pred_score)\n",
        "    # Calculate the final results\n",
        "    answer_info, answer_pred = calculate_answer_BTS(answer_vector_reordered, info_score, pred_score)\n",
        "\n",
        "    LProbTruth = answer_vector_reordered[0]\n",
        "    LProbTruthwithInfo = answer_info[0]\n",
        "    LProbTruthwithPred = answer_pred[0]\n",
        "    switch = compare_vector_probabilities(answer_vector_reordered, answer_info)\n",
        "    ic(answer_vector)\n",
        "    if improvement_round:\n",
        "      prompt_improvement = generate_prompt_answer_improvement(question, answer_choices, answer_vector, COT, info_score = False)\n",
        "      output_improv = call_llm(prompt_improvement, model)\n",
        "      answer_improv = output_improv.choices[0].message.content\n",
        "      answer_vector_improv = extract_probabilities(answer_improv)\n",
        "      ic(answer_improv)\n",
        "      ic(answer_vector_improv)\n",
        "      prompt_improvement_InfoScore = generate_prompt_answer_improvement(question, answer_choices, answer_vector, COT, info_score = True) #incl infoscore\n",
        "      output_IS_improv = call_llm(prompt_improvement_InfoScore, model)\n",
        "      answer_IS_improv = output_IS_improv.choices[0].message.content\n",
        "      answer_IS_vector_improv = extract_probabilities(answer_IS_improv)\n",
        "      ic(answer_IS_improv)\n",
        "      ic(answer_IS_vector_improv)\n",
        "      # recompare\n",
        "      return answer_vector[target], answer_vector_improv[target], answer_IS_vector_improv[target]\n",
        "    return LProbTruth, LProbTruthwithInfo, LProbTruthwithPred, switch, answer_pred_vector\n"
      ],
      "metadata": {
        "id": "Kt0UYnVhfuYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt_answer_improvement(question, answer_choices, answer_vector, COT, info_score):\n",
        "    # Joining the answer choices into a formatted string\n",
        "    answer_choice_text = \"\\n\".join(\n",
        "        f\"({chr(97 + i)}) {answer}\" for i, answer in enumerate(answer_choices)\n",
        "    )\n",
        "\n",
        "    # Constructing the prompt\n",
        "    if not info_score:\n",
        "      prompt_improvement = (\n",
        "          f\"\"\"You are asked the following question:  Question: {question}\\n\n",
        "              {answer_choice_text}. You provide us with an answer in the form of probabilities. {answer_vector} over the options.\n",
        "          Pleas, provide an updated and improved probability distribution over the answers in the following format:\\n\n",
        "          (a) p%\\n\n",
        "          (b) q%\\n\n",
        "          (b) r%\\n\n",
        "          ...\n",
        "          Think step-by-step for {COT} sentences before you output the refined and improved probability distribution. .\"\"\"\n",
        "      )\n",
        "\n",
        "    if info_score:\n",
        "      prompt_improvement = (\n",
        "        f\"\"\"You are asked the following question:  Question: {question}\\n\n",
        "            {answer_choice_text}. You provided us already with an answer in the form of probabilities: {answer_vector} over the options.\n",
        "        We calculate the information score for each of your answers and got {info_score}. Bayesian theory tells us that higher info_scores generally (at least weakly) indicate that this answer is the correct one.\n",
        "        Please provide an updated and improved probability distribution over the answers in the following format:\\n\n",
        "        (a) p%\\n\n",
        "        (b) q%\\n\n",
        "        (b) r%\\n\n",
        "        ...\n",
        "        Think step-by-step for {COT} sentences before you output the refined and improved probability distribution. .\"\"\"\n",
        "      )\n",
        "\n",
        "    return prompt_improvement"
      ],
      "metadata": {
        "id": "JPpewWSygqzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "questions for stewy:\n",
        "\n",
        "x\n",
        "\n",
        "- this is the pseudo, small version of in-context.\n",
        "- interested to hear what you think.\n",
        "\n",
        "\n",
        "\n",
        "TODOS:\n",
        "- I am not tracking the right variable. It should be switch not Probability assigned to correct variable.\n",
        "- if there is an error (in terms of  dimensions, then this throws an error. I know why this is the case but dont know how to fix it right now)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F-LiaR4dnezr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "improvement_round = True\n",
        "\n",
        "results_MMLU = {\"LProbTruth\": [], \"LProbTruthwithInfo\": [], \"LProbTruthwithPred\": [], \"switch\": []}\n",
        "\n",
        "\n",
        "question_indices = list(range(4,5))  # List of question indices from 0 to 20\n",
        "print(question_indices)\n",
        "for question_idx in question_indices:\n",
        "    ic(question_idx)\n",
        "    improvement_round = True\n",
        "    COT = 1\n",
        "    model = \"openai\"\n",
        "    answer_vector, answer_improv, answer_IS_improv = process_MMLU_question(question_idx, dataset_MMLU, model, COT, improvement_round)\n",
        "\n",
        "\n",
        "ic(answer_vector, answer_improv, answer_IS_improv)"
      ],
      "metadata": {
        "id": "f6M-VjSPgKkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T17y2n2LoAmS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYWSgF-vxcTG"
      },
      "source": [
        "**TODOS:**\n",
        "\n",
        "- Fix the None problem (Brendon later?)\n",
        "- batching (Brendon)\n",
        "- second round, in context learning works here (Charlotte writes high-level pseudo code/plan/intention, then person ? implements)\n",
        "- Open source model impelmentation\n",
        "\n",
        "\n",
        "**TODOs (deprioritised)**\n",
        "- do it with Log-probs\n",
        "- think of. a better statistical test\n",
        "\n",
        "\n",
        "Other todos:\n",
        "- slides\n",
        "- theory\n",
        "\n",
        "\n",
        "**Later (on Monday)**\n",
        "- run the much bigger run-throughs (varying model, question, COT and saving results)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}